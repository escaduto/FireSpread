{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import time\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import io\n",
    "import requests\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up veg columns + encode \n",
    "#EVC_Cat, EVH_Cat, EVT_Cat\n",
    "\n",
    "spreadFile = pd.read_csv(r\"F:\\DriversFireProject\\MAJORCSV\\FireDrivers_FULLDATASET_filt.csv\", index_col=0)\n",
    "FRPFile = pd.read_csv(r'F:\\DriversFireProject\\MAJORCSV\\FRPDrivers_FULLDATASET.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spreadFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FRPFile\n",
    "FRPFile['Year'] = pd.DatetimeIndex(FRPFile['ACQ_DATE']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cat columns to FRP file \n",
    "# read in metadata \n",
    "metaPath = \"F:\\DriversFireProject\\CSV(new)\\Vegetation_Metadata\\\\\"\n",
    "EVC12 = pd.read_csv(metaPath + \"EVC_2012.csv\", index_col=None)\n",
    "EVH12 = pd.read_csv(metaPath + \"EVH_2012.csv\", index_col=None)\n",
    "EVT12 = pd.read_csv(metaPath + \"EVT_2012.csv\", index_col=None)\n",
    "EVC14 = pd.read_csv(metaPath + \"EVC_2014.csv\", index_col=None)\n",
    "EVH14 = pd.read_csv(metaPath + \"EVH_2014.csv\", index_col=None)\n",
    "EVT14 = pd.read_csv(metaPath + \"EVT_2014.csv\", index_col=None)\n",
    "len(EVT12['EVT_PHYS'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_pivDF = FRPFile.reset_index()\n",
    "\n",
    "EVC_cat = []\n",
    "EVH_cat = [] \n",
    "EVT_cat = []\n",
    "\n",
    "EVC12['VALUE'] = EVC12['VALUE'].astype('int64')\n",
    "EVH12['VALUE'] = EVH12['VALUE'].astype('int64')\n",
    "EVT12['VALUE'] = EVT12['VALUE'].astype('int64')\n",
    "\n",
    "EVC14['VALUE'] = EVC14['VALUE'].astype('int64')\n",
    "EVH14['VALUE'] = EVH14['VALUE'].astype('int64')\n",
    "EVT14['VALUE'] = EVT14['VALUE'].astype('int64')\n",
    "\n",
    "veg_pivDF['EVC'] = veg_pivDF['EVC'].astype('int64')\n",
    "veg_pivDF['EVH'] = veg_pivDF['EVH'].astype('int64')\n",
    "veg_pivDF['EVT'] = veg_pivDF['EVT'].astype('int64')\n",
    "\n",
    "veg_pivDF = veg_pivDF.replace({'EVT': 0, 'EVH':0, 'EVC':0}, -9999)\n",
    "#veg_pivDF['EVH_Cat'][index] = EVH_Val\n",
    "# add new column that matched layer, value, and year \n",
    "for index, row in veg_pivDF.iterrows():\n",
    "    yr = int(row['year'])\n",
    "    if yr == 2012 or yr == 2013: \n",
    "        EVC = int(row['EVC'])\n",
    "        EVC_Val = EVC12['CLASSNAMES'].loc[EVC12['VALUE'] == EVC].item()\n",
    "        EVC_cat.append(EVC_Val)\n",
    "        EVH = int(row['EVH'])\n",
    "        EVH_Val = EVH12['CLASSNAMES'].loc[EVH12['VALUE'] == EVH].item()\n",
    "        EVH_cat.append(EVH_Val)\n",
    "        EVT = int(row['EVT'])\n",
    "        EVT_Val = EVT12['CLASSNAME'].loc[EVT12['VALUE'] == EVT].item()\n",
    "        EVT_cat.append(EVT_Val)\n",
    "        print(row[0], yr, EVC_Val, EVH_Val, EVT_Val)\n",
    "    else: \n",
    "        EVC = int(row['EVC'])\n",
    "        EVC_Val = EVC14['CLASSNAMES'].loc[EVC14['VALUE'] == EVC].item()\n",
    "        EVC_cat.append(EVC_Val)\n",
    "        EVH = int(row['EVH'])\n",
    "        EVH_Val = EVH14['CLASSNAMES'].loc[EVH14['VALUE'] == EVH].item()\n",
    "        EVH_cat.append(EVH_Val)\n",
    "        EVT = int(row['EVT'])\n",
    "        EVT_Val = EVT14['CLASSNAME'].loc[EVT14['VALUE'] == EVT].item()\n",
    "        EVT_cat.append(EVT_Val)\n",
    "        print(row[0], yr, EVC_Val, EVH_Val, EVT_Val)\n",
    "\n",
    "veg_pivDF['EVC_Cat'] = EVC_cat\n",
    "veg_pivDF['EVH_Cat'] = EVH_cat\n",
    "veg_pivDF['EVT_Cat'] = EVT_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVT_cat = []\n",
    "for index, row in veg_pivDF.iterrows():\n",
    "    yr = int(row['year'])\n",
    "    if yr == 2012 or yr == 2013: \n",
    "        EVT = int(row['EVT'])\n",
    "        EVT_Val = EVT12['EVT_PHYS'].loc[EVT12['VALUE'] == EVT].item()\n",
    "        EVT_cat.append(EVT_Val)\n",
    "        print(row[0], yr, EVC_Val, EVH_Val, EVT_Val)\n",
    "    else: \n",
    "        EVT = int(row['EVT'])\n",
    "        EVT_Val = EVT14['EVT_PHYS'].loc[EVT14['VALUE'] == EVT].item()\n",
    "        EVT_cat.append(EVT_Val)\n",
    "        print(row[0], yr, EVC_Val, EVH_Val, EVT_Val)\n",
    "\n",
    "veg_pivDF['EVT_Phys'] = EVT_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_pivDF.to_csv(r'F:\\DriversFireProject\\MAJORCSV\\FRPDrivers_FULLDATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVT_cat = []\n",
    "for index, row in spreadFile.iterrows():\n",
    "    yr = int(row['Year'])\n",
    "    if yr == 2012 or yr == 2013: \n",
    "        EVT = int(row['EVT'])\n",
    "        EVT_Val = EVT12['EVT_PHYS'].loc[EVT12['VALUE'] == EVT].item()\n",
    "        EVT_cat.append(EVT_Val)\n",
    "        print(row[0], yr, EVC_Val, EVH_Val, EVT_Val)\n",
    "    else: \n",
    "        EVT = int(row['EVT'])\n",
    "        EVT_Val = EVT14['EVT_PHYS'].loc[EVT14['VALUE'] == EVT].item()\n",
    "        EVT_cat.append(EVT_Val)\n",
    "        print(row[0], yr, EVC_Val, EVH_Val, EVT_Val)\n",
    "\n",
    "spreadFile['EVT_Phys'] = EVT_cat\n",
    "\n",
    "spreadFile.to_csv(r\"F:\\DriversFireProject\\MAJORCSV\\FireDrivers_FULLDATASET_filt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical vegetation type and height \n",
    "spreadFile['EVC_Cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spreadFile['EVT_Phys'].unique())\n",
    "spreadFile['EVT_Phys'].unique()\n",
    "spreadFile = spreadFile.drop(columns = [ 'EVC_Cat', 'EVT_Cat', 'EVH_Cat', 'EVT_Phys'])\n",
    "spreadFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoregion = pd.read_csv(r\"F:\\DriversFireProject\\CSV\\Merged_ALL_Ecoregiontbl.csv\")\n",
    "ecoregion = ecoregion[['NAME', 'YEAR', 'NA_L3NAME', 'NA_L2NAME']]\n",
    "ecoregion.columns = ['Fire', 'Year', 'Ecoregion', 'MainEco']\n",
    "ecoregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(spreadFile, ecoregion, on=['Fire','Year'])\n",
    "merged.groupby('Ecoregion')['Fire'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = merged.loc[(merged['Ecoregion'] == 'California Coastal Sage, Chaparral, and Oak Woodlands') \n",
    "                    & (merged['Hectare'] > 5) & (merged['Year'] == 2012)]\n",
    "ax = sns.regplot(x='Hectare', y='energyrel_Mean', data=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = merged.loc[(merged['Ecoregion'] == 'California Coastal Sage, Chaparral, and Oak Woodlands') \n",
    "                    & (merged['Hectare'] > 5) & (merged['Year'] > 2016)].reset_index()\n",
    "sample = sample.drop(columns = [ 'index'])\n",
    "sample = sample.drop_duplicates()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"EVT_Phys\", y=\"Hectare\",\n",
    "            data=spreadFile)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"EVC_Cat\", y=\"Hectare\",\n",
    "            data=spreadFile)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"EVH_Cat\", y=\"Hectare\",\n",
    "            data=spreadFile)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_ylim([0, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"EVT_Phys\", y=\"FRP\",\n",
    "            data=FRPFile)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"EVH_Cat\", y=\"FRP\",\n",
    "            data=FRPFile)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"EVC_Cat\", y=\"FRP\",\n",
    "            data=FRPFile)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.iloc[:, 6:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spreadFile['EVH_Cat'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPFile.corr(method ='kendall') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPFile['Year'] = FRPFile['year']\n",
    "mergedFRP = pd.merge(FRPFile, ecoregion, on=['Fire','Year'])\n",
    "mergedFRP.groupby('Ecoregion')['Fire'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = mergedFRP.loc[(mergedFRP['Ecoregion'] == 'California Coastal Sage, Chaparral, and Oak Woodlands') \n",
    "                    &  (mergedFRP['year'] > 2016)].reset_index()\n",
    "sample = sample.drop(columns = [ 'index'])\n",
    "sample = sample.drop_duplicates()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip encoding for now \n",
    "sample = sample.dropna()\n",
    "columns = sample.iloc[:, 21:-7].columns\n",
    "X = sample.iloc[:, 21:-7].values\n",
    "Y = sample['FRP'].values\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators = 100, random_state =42)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)\n",
    "regressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = regressor.predict(X_test)\n",
    "errors = abs(rf_predictions - Y_test)\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame(regressor.feature_importances_,\n",
    "                                   index = columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / Y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test), len(Y_test), len(Y_train), len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "sc = preprocessing.StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(Y_train)\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "testing_scores_encoded = lab_enc.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifer = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state = 0)\n",
    "classifer.fit(X_train, training_scores_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifer.predict(X_test)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(testing_scores_encoded, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
